{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for item in counts:\\n    print item;'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 55127 word, max count\n",
    "textFile = sc.textFile(\"hdfs://student13-x1:9000/ngram/complete/*\");\n",
    "counts = textFile.map(lambda line: (line.split(\"\\t\")[0],int(line.split(\"\\t\")[2])))\\\n",
    "                    .reduceByKey(lambda a, b: max(a,b))\\\n",
    "                    .sortBy(lambda a: a[1],ascending=False)\\\n",
    "                    .take(55127);\n",
    "counts=sc.parallelize(counts).saveAsTextFile(\"hdfs://student13-x1:9000/ngram_results_1/top_55127\");\n",
    "print \"finished\"\n",
    "\"\"\"for item in counts:\n",
    "    print item;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5512730\n"
     ]
    }
   ],
   "source": [
    "#total words number\n",
    "textFile = sc.textFile(\"hdfs://student13-x1:9000/ngram/complete/*\");\n",
    "counts = textFile.map(lambda line: (line.split(\"\\t\")[0],int(line.split(\"\\t\")[2])))\\\n",
    "                    .reduceByKey(lambda a, b: max(a,b))\\\n",
    "                    .sortBy(lambda a: a[1],ascending=False);\n",
    "print len(counts.collect());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55127\n",
      "11025\n",
      "(u'Zinik', (False, {u'1985': u'16'}))\n"
     ]
    }
   ],
   "source": [
    "textFile = sc.textFile(\"hdfs://student13-x1:9000/ngram_results/top_55127/*\");\n",
    "print len(textFile.collect())\n",
    "newlist=[]\n",
    "count=0\n",
    "for item in textFile.collect():\n",
    "    count+=1\n",
    "    newlist.append(item)\n",
    "    if count>=11025:\n",
    "        break\n",
    "newlist2=sc.parallelize(newlist).map(lambda line:line.replace('(','').replace(')','').split(',')[0]).collect();\n",
    "print len(newlist2)\n",
    "def isPopular_word(word):\n",
    "    for w in newlist2:\n",
    "        if word in w:\n",
    "            return True\n",
    "    return False\n",
    "def isPopular_line(line):\n",
    "    word=line.split(\"\\t\")[0]\n",
    "    for w in newlist2:\n",
    "        if word in w:\n",
    "            return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "(u'Zinik', {1985: 16, 1987: 5, 1988: 4, 1991: 8, 1992: 10, 1993: 16, 1995: 44, 1996: 76, 1997: 10, 1998: 1, 1999: 2, 2000: 2, 2001: 10, 2002: 10, 2003: 16, 2004: 16, 2005: 14, 2006: 3, 2007: 1, 2008: 4, 2009: 7})\n"
     ]
    }
   ],
   "source": [
    "print \"start\";\n",
    "textFile = sc.textFile(\"hdfs://student13-x1:9000/ngram/complete/googlebooks-eng-fiction-all-1gram-20120701-z\");\n",
    "\n",
    "def reducefunction(a,b):\n",
    "    a.update(b)\n",
    "    return a\n",
    "def mapfunction(line):\n",
    "    word=line.split(\"\\t\")[0]\n",
    "    year=int(line.split(\"\\t\")[1])\n",
    "    wcount=int(line.split(\"\\t\")[2])\n",
    "    a=word\n",
    "    b={year:wcount}\n",
    "    return (a,b)\n",
    "p_counts=textFile.map(mapfunction);\n",
    "p_counts=p_counts.reduceByKey(reducefunction)\\\n",
    "                    .take(5);\n",
    "#p_counts = textFile.filter(isPopular).take(5);\n",
    "#print len(p_counts)\n",
    "\n",
    "print p_counts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step1\n",
    "#11025 detail info (word,(max,dic))\n",
    "textFile = sc.textFile(\"hdfs://student13-x1:9000/ngram/complete/*\");\n",
    "def mapfunction(line):\n",
    "    word=line.split(\"\\t\")[0]\n",
    "    year=int(line.split(\"\\t\")[1])\n",
    "    wcount=int(line.split(\"\\t\")[2])\n",
    "    a=word\n",
    "    tempMax=wcount\n",
    "    b=(tempMax,{year:wcount})\n",
    "    return (a,b)\n",
    "def reducefuntion(a,b):\n",
    "    tempMax=max(a[0],b[0])\n",
    "    a[1].update(b[1])\n",
    "    return (tempMax,a[1])\n",
    "counts = textFile.map(mapfunction);\n",
    "counts = counts.reduceByKey(reducefuntion);\n",
    "counts = counts.sortBy(lambda a: a[1][0],ascending=False).take(11025);\n",
    "\n",
    "counts=sc.parallelize(counts).saveAsTextFile(\"hdfs://student13-x1:9000/cache1_ngram_results_step1/top_11025_detailed\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PipelinedRDD' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-51980d5e7d6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcounts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://student13-x1:9000/ngram_results_2/top_11025_detailed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.1.0-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# Make sure we distribute data evenly if it's smaller than self.batchSize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"__len__\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                 \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Make it a list so we can compute its length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m             \u001b[0mbatchSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnumSlices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batchSize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbatched_serializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'PipelinedRDD' object is not iterable"
     ]
    }
   ],
   "source": [
    "counts=sc.parallelize(counts).saveAsTextFile(\"hdfs://student13-x1:9000/ngram_results_2/top_11025_detailed\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o174.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://student13-x1:9000/ngram_results_1/11025_formatted_unit10000 already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1191)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1071)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1488)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1467)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1467)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1467)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e5eedd91f88d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#counts=counts.take(5);\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mcounts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://student13-x1:9000/ngram_results_1/11025_formatted_unit10000\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"finished\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#print counts[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.1.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o174.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://student13-x1:9000/ngram_results_1/11025_formatted_unit10000 already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1191)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1071)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1488)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1467)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1467)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1467)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "#step2\n",
    "#get study formatted source\n",
    "unit=10000\n",
    "textFile = sc.textFile(\"hdfs://student13-x1:9000/ngram_results_1/top_11025_detailed/*\");\n",
    "#print textFile.collect()[0];\n",
    "def find_nearest(array,value):\n",
    "    minabs=9999999\n",
    "    result=0\n",
    "    for v in array:\n",
    "        tempabs=abs(v-value)\n",
    "        if tempabs<minabs:\n",
    "            minabs=tempabs\n",
    "            result=v\n",
    "    return result\n",
    "def get2counts(dic):\n",
    "    year4=-1\n",
    "    for key in dic.keys():\n",
    "        if dic[key]>=89794:\n",
    "            year4=key\n",
    "            break\n",
    "    year3=year4-5\n",
    "    year2=year3-10\n",
    "    year1=year2-10\n",
    "    year3=find_nearest(dic.keys(),year3)\n",
    "    year2=find_nearest(dic.keys(),year2)\n",
    "    year1=find_nearest(dic.keys(),year1)\n",
    "    value3=dic[year3]/unit\n",
    "    value2=dic[year2]/unit\n",
    "    value1=dic[year1]/unit\n",
    "    return (value1,value2,value3)\n",
    "def mapfunction(line):\n",
    "    line=line.replace('(','').replace(')','')\n",
    "    word=line.split(',')[0].replace('u\\'','').replace('\\'','')\n",
    "    maxcount=int(line.split(',')[1])\n",
    "    dic_string='{'+line.split('{')[1]\n",
    "    import ast\n",
    "    dic= ast.literal_eval(dic_string)\n",
    "    value_2=get2counts(dic)\n",
    "    return (word,value_2)\n",
    "    #return (word,(maxcount,dic))\n",
    "counts=textFile.map(mapfunction);\n",
    "counts=counts.reduceByKey(lambda a,b:a);\n",
    "#counts=counts.take(5);\n",
    "counts=counts.saveAsTextFile(\"hdfs://student13-x1:9000/ngram_results_1/11025_formatted_unit10000\");\n",
    "print \"finished\"\n",
    "#print counts[0]\n",
    "#print counts[1]\n",
    "#for item in counts:\n",
    "#    if 'kill' in item[0]:\n",
    "#        print item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'kill_VERB', (1, 2, 6))\n"
     ]
    }
   ],
   "source": [
    "textFile = sc.textFile(\"hdfs://student13-x1:9000/ngram_results/11025_formatted_unit10000\");\n",
    "print textFile.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o835.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://student13-x1:9000/ngram_results/year_counts already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1191)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1071)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1488)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1467)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1467)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1467)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-12de1c3570d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtextFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://student13-x1:9000/ngram/complete/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;34m.\u001b[0m\u001b[0msortBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcounts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://student13-x1:9000/ngram_results/year_counts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.1.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o835.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://student13-x1:9000/ngram_results/year_counts already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1191)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1168)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1071)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1037)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1488)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1467)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1467)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1467)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "#get total number and compute frequency\n",
    "textFile = sc.textFile(\"hdfs://student13-x1:9000/ngram/complete/*\");\n",
    "counts = textFile.map(lambda line: (int(line.split(\"\\t\")[1]),int(line.split(\"\\t\")[2])))\\\n",
    "                    .reduceByKey(lambda a, b: a+b)\\\n",
    "                    .sortBy(lambda a: a[1],ascending=False).collect();\n",
    "#counts=sc.parallelize(counts).saveAsTextFile(\"hdfs://student13-x1:9000/ngram_results/year_counts\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2008, 13790808567)\n",
      "(2007, 10401776121)\n",
      "(2006, 8822750185)\n",
      "(2004, 8561042762)\n",
      "(2005, 8238348598)\n",
      "(2003, 7430168916)\n",
      "(2002, 6669648588)\n",
      "(2001, 5910085555)\n",
      "(2000, 5706074792)\n",
      "(2009, 5541934147)\n",
      "(1999, 4914319741)\n",
      "(1998, 4545440716)\n",
      "(1997, 4293207657)\n",
      "(1996, 4228032019)\n",
      "(1995, 4005306672)\n",
      "(1994, 3915798737)\n",
      "(1993, 3640121489)\n",
      "(1992, 3637111283)\n",
      "(1990, 3392380200)\n",
      "(1991, 3367310379)\n",
      "(1989, 3122962323)\n",
      "(1988, 2946975237)\n",
      "(1987, 2841397746)\n",
      "(1986, 2692115243)\n",
      "(1985, 2525988715)\n",
      "(1984, 2421380424)\n",
      "(1983, 2261789542)\n",
      "(1982, 2191301282)\n",
      "(1981, 2069121437)\n",
      "(1980, 1992066746)\n",
      "(1979, 1908817049)\n",
      "(1978, 1858504180)\n",
      "(1977, 1809766266)\n",
      "(1976, 1785546223)\n",
      "(1970, 1715307924)\n",
      "(1972, 1681296079)\n",
      "(1975, 1655844684)\n",
      "(1971, 1651130374)\n",
      "(1969, 1638727902)\n",
      "(1974, 1633939711)\n",
      "(1968, 1632327388)\n",
      "(1973, 1629469672)\n",
      "(1967, 1492931678)\n",
      "(1966, 1392438269)\n",
      "(1965, 1357600303)\n",
      "(1963, 1218518917)\n",
      "(1964, 1208129610)\n",
      "(1962, 1171119682)\n",
      "(1961, 1104374389)\n",
      "(1960, 985190888)\n",
      "(1959, 880749557)\n",
      "(1958, 857664234)\n",
      "(1957, 848561500)\n",
      "(1955, 784698816)\n",
      "(1956, 781570046)\n",
      "(1950, 773771561)\n",
      "(1952, 764933625)\n",
      "(1954, 736498872)\n",
      "(1951, 735380487)\n",
      "(1953, 712706706)\n",
      "(1949, 711630595)\n",
      "(1948, 669428137)\n",
      "(1947, 647751489)\n",
      "(1908, 618556771)\n",
      "(1906, 617786733)\n",
      "(1900, 614725922)\n",
      "(1912, 613254294)\n",
      "(1922, 609311761)\n",
      "(1904, 595598629)\n",
      "(1901, 591144975)\n",
      "(1907, 588725604)\n",
      "(1910, 587356102)\n",
      "(1930, 585925911)\n",
      "(1913, 579044392)\n",
      "(1911, 578672404)\n",
      "(1920, 578147069)\n",
      "(1902, 572005965)\n",
      "(1899, 570737566)\n",
      "(1946, 570254056)\n",
      "(1903, 569936348)\n",
      "(1905, 567946916)\n",
      "(1914, 567248131)\n",
      "(1927, 559007855)\n",
      "(1928, 558460724)\n",
      "(1909, 555038044)\n",
      "(1929, 552535762)\n",
      "(1937, 549612842)\n",
      "(1931, 546777773)\n",
      "(1938, 543970065)\n",
      "(1923, 539647324)\n",
      "(1925, 539177448)\n",
      "(1936, 538798417)\n",
      "(1935, 527812925)\n",
      "(1939, 522772301)\n",
      "(1921, 516854826)\n",
      "(1916, 506188589)\n",
      "(1926, 503289295)\n",
      "(1917, 500982799)\n",
      "(1898, 499566802)\n",
      "(1924, 498660820)\n",
      "(1915, 496162881)\n",
      "(1940, 494778539)\n",
      "(1932, 494721562)\n",
      "(1919, 493662386)\n",
      "(1934, 488558050)\n",
      "(1941, 471754520)\n",
      "(1896, 467204588)\n",
      "(1897, 459214304)\n",
      "(1895, 455140614)\n",
      "(1942, 440984022)\n",
      "(1918, 431218534)\n",
      "(1933, 429088481)\n",
      "(1945, 421101650)\n",
      "(1894, 416235512)\n",
      "(1892, 415644012)\n",
      "(1893, 410527910)\n",
      "(1883, 391844363)\n",
      "(1943, 384113087)\n",
      "(1891, 374334682)\n",
      "(1944, 371125262)\n",
      "(1884, 360926150)\n",
      "(1885, 353998978)\n",
      "(1890, 349963647)\n",
      "(1882, 339802893)\n",
      "(1887, 337781927)\n",
      "(1888, 337018019)\n",
      "(1880, 333724669)\n",
      "(1881, 324488876)\n",
      "(1889, 323838960)\n",
      "(1886, 304604479)\n",
      "(1854, 274708370)\n",
      "(1876, 273525431)\n",
      "(1856, 271391415)\n",
      "(1877, 266469720)\n",
      "(1874, 262916252)\n",
      "(1875, 262367393)\n",
      "(1879, 258178313)\n",
      "(1853, 253203809)\n",
      "(1860, 252067904)\n",
      "(1878, 251607404)\n",
      "(1855, 248310459)\n",
      "(1873, 243733136)\n",
      "(1857, 237085668)\n",
      "(1872, 235685888)\n",
      "(1859, 233359863)\n",
      "(1870, 231630474)\n",
      "(1866, 230808185)\n",
      "(1871, 230547853)\n",
      "(1868, 229369894)\n",
      "(1852, 228455887)\n",
      "(1869, 227078323)\n",
      "(1867, 219600061)\n",
      "(1858, 216946386)\n",
      "(1865, 211431411)\n",
      "(1851, 210936745)\n",
      "(1864, 200573335)\n",
      "(1850, 200222895)\n",
      "(1849, 194565258)\n",
      "(1848, 187533936)\n",
      "(1847, 182451487)\n",
      "(1846, 181839400)\n",
      "(1845, 180693306)\n",
      "(1861, 175542480)\n",
      "(1840, 174710740)\n",
      "(1863, 170994198)\n",
      "(1839, 168771344)\n",
      "(1844, 167332505)\n",
      "(1862, 163947654)\n",
      "(1841, 159415449)\n",
      "(1843, 154586418)\n",
      "(1836, 151264660)\n",
      "(1842, 145437093)\n",
      "(1838, 142946257)\n",
      "(1835, 137090652)\n",
      "(1837, 135196156)\n",
      "(1825, 125215848)\n",
      "(1824, 125032043)\n",
      "(1832, 124598045)\n",
      "(1830, 123085952)\n",
      "(1831, 122564180)\n",
      "(1833, 120995240)\n",
      "(1834, 117188805)\n",
      "(1829, 112971524)\n",
      "(1822, 108529788)\n",
      "(1828, 105812178)\n",
      "(1827, 98348014)\n",
      "(1823, 94951654)\n",
      "(1826, 94465986)\n",
      "(1820, 88303790)\n",
      "(1812, 76746815)\n",
      "(1818, 73677246)\n",
      "(1811, 68156750)\n",
      "(1821, 67591731)\n",
      "(1817, 66869413)\n",
      "(1819, 65934260)\n",
      "(1814, 63955851)\n",
      "(1815, 61611988)\n",
      "(1816, 59649511)\n",
      "(1810, 58325949)\n",
      "(1813, 53701547)\n",
      "(1809, 51818327)\n",
      "(1808, 49726770)\n",
      "(1806, 49062244)\n",
      "(1807, 48421324)\n",
      "(1805, 42959235)\n",
      "(1804, 41760246)\n",
      "(1801, 38135028)\n",
      "(1803, 37132421)\n",
      "(1802, 35120745)\n",
      "(1800, 30131435)\n",
      "(1796, 23480168)\n",
      "(1799, 18867924)\n",
      "(1797, 18699137)\n",
      "(1794, 18555827)\n",
      "(1798, 18247666)\n",
      "(1792, 18082723)\n",
      "(1795, 17817088)\n",
      "(1789, 16996769)\n",
      "(1793, 16214039)\n",
      "(1790, 15628039)\n",
      "(1791, 15606273)\n",
      "(1788, 15312459)\n",
      "(1787, 14295848)\n",
      "(1784, 10752986)\n",
      "(1766, 10521549)\n",
      "(1786, 10117310)\n",
      "(1768, 9817319)\n",
      "(1785, 9797597)\n",
      "(1775, 9538539)\n",
      "(1751, 9285901)\n",
      "(1776, 9061824)\n",
      "(1777, 8671844)\n",
      "(1770, 8420964)\n",
      "(1781, 8319827)\n",
      "(1755, 8295500)\n",
      "(1750, 8096830)\n",
      "(1767, 7843375)\n",
      "(1754, 7798016)\n",
      "(1771, 7782013)\n",
      "(1783, 7667269)\n",
      "(1773, 7544687)\n",
      "(1753, 7445620)\n",
      "(1763, 7318119)\n",
      "(1782, 7254561)\n",
      "(1774, 7170869)\n",
      "(1759, 7074495)\n",
      "(1779, 7038340)\n",
      "(1772, 6945413)\n",
      "(1778, 6943746)\n",
      "(1764, 6847533)\n",
      "(1780, 6678822)\n",
      "(1761, 6208137)\n",
      "(1769, 6092366)\n",
      "(1752, 6080143)\n",
      "(1747, 5904560)\n",
      "(1758, 5837162)\n",
      "(1765, 5832101)\n",
      "(1737, 5816376)\n",
      "(1748, 5601315)\n",
      "(1749, 5597044)\n",
      "(1727, 5256297)\n",
      "(1760, 5252936)\n",
      "(1726, 5152474)\n",
      "(1739, 5114449)\n",
      "(1744, 5084700)\n",
      "(1738, 5072052)\n",
      "(1757, 4951865)\n",
      "(1722, 4700236)\n",
      "(1756, 4493426)\n",
      "(1720, 4446190)\n",
      "(1731, 4390473)\n",
      "(1728, 4350889)\n",
      "(1740, 4322380)\n",
      "(1729, 4288895)\n",
      "(1742, 4142186)\n",
      "(1762, 4102493)\n",
      "(1730, 3986438)\n",
      "(1725, 3938669)\n",
      "(1734, 3658228)\n",
      "(1732, 3583642)\n",
      "(1745, 3576454)\n",
      "(1724, 3435163)\n",
      "(1743, 3350116)\n",
      "(1741, 3310686)\n",
      "(1735, 3166697)\n",
      "(1736, 3102344)\n",
      "(1714, 2814965)\n",
      "(1717, 2759484)\n",
      "(1708, 2725842)\n",
      "(1723, 2668190)\n",
      "(1704, 2636013)\n",
      "(1715, 2531013)\n",
      "(1733, 2518914)\n",
      "(1746, 2497971)\n",
      "(1706, 2378107)\n",
      "(1721, 2359918)\n",
      "(1710, 2316107)\n",
      "(1711, 2215584)\n",
      "(1719, 2166858)\n",
      "(1702, 1834481)\n",
      "(1705, 1828731)\n",
      "(1718, 1750688)\n",
      "(1707, 1747418)\n",
      "(1716, 1701098)\n",
      "(1713, 1657796)\n",
      "(1683, 1442684)\n",
      "(1703, 1337416)\n",
      "(1701, 1270961)\n",
      "(1712, 1258273)\n",
      "(1700, 1235583)\n",
      "(1675, 1208873)\n",
      "(1682, 1206075)\n",
      "(1709, 1200301)\n",
      "(1685, 1173381)\n",
      "(1698, 1034830)\n",
      "(1681, 957790)\n",
      "(1699, 927380)\n",
      "(1688, 848687)\n",
      "(1678, 651299)\n",
      "(1694, 648615)\n",
      "(1676, 587274)\n",
      "(1692, 548967)\n",
      "(1644, 536853)\n",
      "(1684, 520837)\n",
      "(1693, 431925)\n",
      "(1587, 424470)\n",
      "(1686, 423257)\n",
      "(1673, 395388)\n",
      "(1695, 378008)\n",
      "(1689, 377736)\n",
      "(1687, 364295)\n",
      "(1680, 352685)\n",
      "(1679, 350738)\n",
      "(1668, 348289)\n",
      "(1696, 341040)\n",
      "(1643, 303333)\n",
      "(1667, 302008)\n",
      "(1697, 297099)\n",
      "(1690, 293923)\n",
      "(1658, 256408)\n",
      "(1677, 255631)\n",
      "(1670, 238229)\n",
      "(1579, 217906)\n",
      "(1581, 217397)\n",
      "(1656, 212760)\n",
      "(1637, 203985)\n",
      "(1651, 187407)\n",
      "(1590, 185447)\n",
      "(1659, 179351)\n",
      "(1650, 166885)\n",
      "(1607, 166462)\n",
      "(1592, 163750)\n",
      "(1648, 159077)\n",
      "(1657, 144216)\n",
      "(1672, 124361)\n",
      "(1631, 122212)\n",
      "(1600, 119146)\n",
      "(1653, 116775)\n",
      "(1621, 113305)\n",
      "(1669, 112554)\n",
      "(1691, 106004)\n",
      "(1647, 104844)\n",
      "(1664, 99737)\n",
      "(1629, 92310)\n",
      "(1655, 91718)\n",
      "(1665, 91118)\n",
      "(1666, 89236)\n",
      "(1515, 86075)\n",
      "(1649, 85421)\n",
      "(1524, 82697)\n",
      "(1635, 80679)\n",
      "(1645, 80585)\n",
      "(1563, 74621)\n",
      "(1662, 73999)\n",
      "(1663, 71545)\n",
      "(1620, 68304)\n",
      "(1660, 66209)\n",
      "(1638, 64787)\n",
      "(1575, 59830)\n",
      "(1572, 57629)\n",
      "(1652, 54506)\n",
      "(1603, 52715)\n",
      "(1582, 49157)\n",
      "(1624, 48053)\n",
      "(1568, 47521)\n",
      "(1630, 45984)\n",
      "(1671, 45634)\n",
      "(1598, 45524)\n",
      "(1584, 44993)\n",
      "(1634, 43587)\n",
      "(1642, 42008)\n",
      "(1623, 40820)\n",
      "(1674, 40772)\n",
      "(1661, 40672)\n",
      "(1609, 33605)\n",
      "(1625, 20692)\n",
      "(1564, 19905)\n",
      "(1646, 19523)\n",
      "(1574, 19060)\n",
      "(1606, 17679)\n",
      "(1619, 17575)\n",
      "(1640, 17467)\n",
      "(1520, 17321)\n",
      "(1612, 16109)\n",
      "(1611, 15912)\n",
      "(1641, 15384)\n",
      "(1632, 14062)\n",
      "(1507, 13569)\n",
      "(1588, 13460)\n",
      "(1654, 13196)\n",
      "(1626, 12972)\n",
      "(1593, 12281)\n",
      "(1589, 11687)\n",
      "(1595, 10476)\n",
      "(1636, 9793)\n",
      "(1505, 9701)\n",
      "(1618, 6169)\n",
      "(1605, 5688)\n",
      "(1594, 4500)\n",
      "(1597, 3227)\n",
      "(1614, 2828)\n",
      "(1628, 2102)\n",
      "(1610, 1960)\n",
      "(1527, 1544)\n",
      "(1541, 1538)\n",
      "(1602, 946)\n",
      "(1525, 898)\n"
     ]
    }
   ],
   "source": [
    "textFile = sc.textFile(\"hdfs://student13-x1:9000/ngram_results/year_counts\");\n",
    "counts=textFile.collect();\n",
    "year_f={}\n",
    "for item in counts:\n",
    "    year_f[item[0]]=item[1]\n",
    "    print item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Zeno', (0.00010983217643440822, {1598: 0.00010983217643440822, 1708: 3.668591209615231e-07, 1717: 2.1743195466978608e-06, 1720: 6.747349978296025e-07, 1726: 1.9408152277915426e-07, 1728: 2.2983808596358123e-07, 1734: 2.733563900336447e-07, 1737: 1.7192836226543815e-07, 1739: 5.865734510208235e-07, 1745: 5.59213120034537e-07, 1748: 5.3558851805335e-07, 1750: 1.2350512484515544e-07, 1753: 2.68614299413615e-07, 1754: 3.8471323988050293e-07, 1760: 1.9036972847184888e-07, 1768: 1.0186080334152328e-07, 1770: 4.7500499942761895e-07, 1788: 1.3061259461984517e-07, 1790: 2.559502187062625e-07, 1794: 4.3113141764039945e-07, 1795: 5.6125894422253516e-08, 1796: 2.9812393165159634e-07, 1797: 1.0695680768583064e-07, 1801: 2.622261087627889e-08, 1804: 4.789243818151838e-08, 1806: 3.2611635130264324e-07, 1807: 6.195617451517847e-08, 1808: 4.021978503731491e-08, 1809: 9.649095772621143e-08, 1810: 1.3716022005917126e-07, 1811: 1.1737648875569917e-07, 1812: 5.211942671497182e-08, 1813: 2.4207868723036973e-07, 1814: 6.254314401977077e-08, 1815: 4.057651897224936e-07, 1817: 4.486356116211159e-08, 1818: 4.0718134334174215e-08, 1819: 1.51666220262425e-08, 1820: 2.604644715702463e-07, 1821: 4.4384127401619586e-08, 1822: 2.3956556516999737e-07, 1823: 1.0531675414522005e-08, 1824: 3.199179909425298e-08, 1825: 3.6736563889261046e-07, 1826: 1.482014912753888e-07, 1827: 1.3218365548286517e-07, 1828: 4.7253540135994554e-08, 1829: 6.196251720920397e-08, 1830: 4.0622019968615104e-08, 1831: 2.447697198316833e-08, 1832: 2.8090328383563324e-07, 1833: 1.6529575874224475e-08, 1834: 8.533238307191544e-09, 1835: 3.6472216938613726e-08, 1836: 6.6109294794964e-08, 1837: 1.4053654010695393e-07, 1838: 1.2592145032520858e-07, 1839: 1.066531768568484e-07, 1840: 6.296121234447293e-08, 1841: 1.5055002605174108e-07, 1842: 2.750330000064014e-08, 1843: 1.2937747221751396e-08, 1844: 1.1952250401080173e-08, 1845: 7.194511123726963e-08, 1846: 2.7496791124475774e-08, 1847: 2.027936335755926e-07, 1848: 5.3323682173449394e-09, 1849: 8.737428343964677e-08, 1850: 1.4983301485077418e-08, 1851: 4.2666819382275e-08, 1852: 7.878982781476758e-08, 1853: 1.974693832508657e-08, 1854: 9.100559986577766e-08, 1855: 2.4163299541079742e-08, 1856: 3.316243441230446e-08, 1857: 1.687153860350597e-08, 1858: 2.7656602677861618e-08, 1859: 8.998976829190203e-08, 1860: 6.744214447865604e-08, 1861: 4.557301457744017e-08, 1862: 1.8298523503117646e-08, 1863: 1.9298900422340645e-07, 1864: 2.1438542665703793e-07, 1865: 2.222943117945706e-07, 1866: 6.498902974346426e-08, 1867: 9.562838873710514e-08, 1868: 6.103678105200677e-08, 1869: 7.046027022138965e-08, 1870: 5.612387599742165e-08, 1871: 6.939990892042703e-08, 1872: 2.9700547874975016e-08, 1873: 1.6411391842921187e-08, 1874: 9.889080573079217e-08, 1875: 6.479463703784259e-08, 1876: 4.021563903504095e-08, 1877: 1.8763858047360877e-07, 1878: 2.3846675036637633e-08, 1879: 4.64795042641711e-08, 1880: 1.408346591243454e-07, 1881: 7.088070408922123e-08, 1882: 2.942882537495053e-08, 1883: 1.2504964885765116e-07, 1884: 2.4935849064968e-08, 1885: 1.1864441032369308e-07, 1886: 2.6263566531469156e-07, 1887: 5.0328329141185815e-08, 1888: 9.495041272555815e-08, 1889: 5.24952278749907e-08, 1890: 7.429343082597376e-08, 1891: 2.6714062257261002e-08, 1892: 6.495943456536551e-07, 1893: 5.846131143677905e-08, 1894: 8.40870108170876e-08, 1895: 3.0759724729817236e-08, 1896: 1.6052924548763206e-07, 1897: 5.2263180373405793e-08, 1898: 8.607457466719336e-08, 1899: 5.256356298789696e-08, 1900: 6.181616658748937e-08, 1901: 5.2440604777195305e-08, 1902: 6.643287365019e-08, 1903: 1.4563029764860688e-07, 1904: 1.0241796577406158e-07, 1905: 9.331858049916763e-08, 1906: 9.064616801992735e-08, 1907: 1.729158699882195e-06, 1908: 1.9399997805536916e-08, 1909: 1.0269566314629056e-07, 1910: 5.448142939357766e-08, 1911: 1.0022942099723836e-07, 1912: 8.968547067360608e-08, 1913: 1.8996816396073482e-08, 1914: 5.993849629801601e-08, 1915: 1.2092803048682717e-08, 1916: 1.1063070408329572e-07, 1917: 9.980382579961593e-09, 1918: 6.957029356256751e-09, 1919: 9.723244338895206e-08, 1920: 8.302385772364782e-08, 1921: 1.9347792643035126e-08, 1922: 2.7900331305109997e-08, 1923: 9.450616677198607e-08, 1924: 3.328916035552984e-07, 1925: 2.5965477695573055e-08, 1926: 8.54379388299924e-08, 1927: 1.377440393927917e-07, 1928: 7.520672125189595e-08, 1929: 7.782301700138642e-08, 1930: 1.0069532494185941e-07, 1931: 7.681365643222663e-08, 1932: 8.08535610178236e-08, 1933: 7.224617153029564e-08, 1934: 6.345202990719321e-08, 1935: 2.8419160065093138e-08, 1936: 4.2687579017144734e-08, 1937: 8.369527872130761e-08, 1938: 2.3163039311731244e-07, 1939: 5.164772492412524e-08, 1940: 1.3743522533825986e-07, 1941: 8.902935365621934e-08, 1942: 9.750920181865456e-08, 1943: 7.029179924817297e-08, 1944: 8.083524101358532e-09, 1945: 3.562085306481226e-08, 1946: 4.033290032399173e-08, 1947: 2.31570289759689e-08, 1948: 9.709780095484693e-08, 1949: 1.320910043222636e-07, 1950: 7.366515244671806e-08, 1951: 2.991648594015522e-08, 1952: 3.5950831681637734e-07, 1953: 4.770545823936726e-08, 1954: 9.640204852886726e-08, 1955: 1.3253492662336322e-07, 1956: 6.781222012185457e-08, 1957: 9.07418024503822e-08, 1958: 9.094468080617199e-08, 1959: 9.08317232341225e-08, 1960: 5.8871839667278776e-08, 1961: 8.602155296812121e-08, 1962: 1.3918304209663177e-07, 1963: 2.9544063286790976e-08, 1964: 1.746501354271087e-07, 1965: 6.18738812995094e-08, 1966: 4.165355211161609e-07, 1967: 1.5271964776408208e-07, 1968: 8.699235278652324e-08, 1969: 6.224340226068843e-08, 1970: 9.735861279680068e-08, 1971: 7.267748318946497e-08, 1972: 1.6653818652009122e-07, 1973: 4.848203152074376e-08, 1974: 2.6745172851729535e-07, 1975: 1.376940737275091e-07, 1976: 3.9763742369384744e-08, 1977: 9.669756989491814e-08, 1978: 3.1261699933330256e-07, 1979: 1.9278956052534713e-07, 1980: 1.4858939872088e-07, 1981: 4.513993153317274e-07, 1982: 2.1311537753209784e-07, 1983: 2.02052397676176e-07, 1984: 4.922811748972825e-07, 1985: 8.66987246219744e-08, 1986: 8.84063193872715e-08, 1987: 9.924692887399792e-08, 1988: 1.7509478651923941e-07, 1989: 1.610650235180567e-07, 1990: 2.0457612622547437e-07, 1991: 2.2213574509342847e-07, 1992: 2.9528928768825906e-07, 1993: 2.4916750793643633e-07, 1994: 7.584659476843791e-08, 1995: 1.8700191054933534e-07, 1996: 2.2705599098728112e-07, 1997: 1.5559461674555567e-07, 1998: 1.4036042704379206e-07, 1999: 2.8020154824516944e-07, 2000: 2.44476290769236e-07, 2001: 2.7952218028424125e-07, 2002: 1.277428621251372e-07, 2003: 1.994571074701527e-07, 2004: 2.3618618154497194e-07, 2005: 1.5452125931027518e-07, 2006: 1.4179252203319638e-07, 2007: 1.7516239330719586e-07, 2008: 2.4306044012683886e-07, 2009: 4.853901054483244e-07}))\n"
     ]
    }
   ],
   "source": [
    "textFile = sc.textFile(\"hdfs://student13-x1:9000/ngram/complete/*\");\n",
    "textFile2 = sc.textFile(\"hdfs://student13-x1:9000/ngram_results/year_counts\");\n",
    "year_c_f=textFile2.collect();\n",
    "year_f={}\n",
    "for item in year_c_f:\n",
    "    item=item.replace('(','').replace(')','');\n",
    "    year=int(item.split(',')[0])\n",
    "    y_count=int(item.split(',')[1])\n",
    "    year_f[year]=y_count\n",
    "def mapfunction(line):\n",
    "    word=line.split(\"\\t\")[0]\n",
    "    year=int(line.split(\"\\t\")[1])\n",
    "    wcount=float(line.split(\"\\t\")[2])/year_f[year]\n",
    "    a=word\n",
    "    tempMax=wcount\n",
    "    b=(tempMax,{year:wcount})\n",
    "    return (a,b)\n",
    "def reducefuntion(a,b):\n",
    "    tempMax=max(a[0],b[0])\n",
    "    a[1].update(b[1])\n",
    "    return (tempMax,a[1])\n",
    "counts2 = textFile.map(mapfunction);\n",
    "counts2 = counts2.reduceByKey(reducefuntion);\n",
    "counts2 = counts2.sortBy(lambda a: a[1][0],ascending=False).take(11025);\n",
    "\n",
    "#print counts2[0]\n",
    "#counts2 =sc.parallelize(counts).saveAsTextFile(\"hdfs://student13-x1:9000/ngram_results/top_11025_detailed_f\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'kill_VERB', (1, 2, 6))\n",
      "(1, 2, 6)\n",
      "372\n",
      "((1, 2, 5), 912)\n"
     ]
    }
   ],
   "source": [
    "#step3\n",
    "#3 conditions ar\n",
    "textFile = sc.textFile(\"hdfs://student13-x1:9000/ngram_results_1/11025_formatted_unit10000\");\n",
    "print textFile.collect()[0]\n",
    "dictionary={}\n",
    "allcounts=textFile.collect();\n",
    "dic_ar={}\n",
    "for item in allcounts:\n",
    "    item=item.replace('(','').replace(')','')\n",
    "    word=item.split(',')[0].replace('u\\'','').replace('\\'','')\n",
    "    counts1=int(item.split(',')[1])\n",
    "    counts2=int(item.split(',')[2])\n",
    "    counts3=int(item.split(',')[3])\n",
    "    counts=(counts1,counts2,counts3)\n",
    "    dictionary[word]=counts\n",
    "    if counts in dic_ar.keys():\n",
    "        dic_ar[counts]+=1\n",
    "    else:\n",
    "        dic_ar[counts]=1\n",
    "print dictionary['kill_VERB']\n",
    "print dic_ar[(1,2,6)]\n",
    "dic_ar_list=[]\n",
    "for key in dic_ar.keys():\n",
    "    dic_ar_list.append((key,dic_ar[key]))\n",
    "dic_ar_rdd=sc.parallelize(dic_ar_list).sortBy(lambda a: a[1],ascending=False);\n",
    "dic_ar_rdd.saveAsTextFile(\"hdfs://student13-x1:9000/ngram_results_2/ar_rules_11025_u10000\");\n",
    "print dic_ar_rdd.collect()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"hdfs://student13-x1:9000/ngram_results/ar_rules_11025_u10000\");\n",
    "print textFile.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Alhamhra_NOUN', (15, {2008: 15, 2009: 6, 2007: 2}))\n",
      "(u'Antam', [4, 5, 6])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "457\n",
      "(u'A32_NUM', [1, 2, 4], 954)\n",
      "(u'Bardachd_NOUN', (16, {2008: 5, 2009: 10, 2007: 6}))\n",
      "(u'Bardachd_NOUN', [6, 5, 10])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "386\n",
      "(u'Brdhman', [1, 2, 4], 954)\n",
      "(u'Chauntecleer_NOUN', (1379, {2008: 111, 2009: 62, 2007: 21}))\n",
      "(u'Chauntecleer_NOUN', [21, 111, 62])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "595\n",
      "(u'corporelles_X', [1, 2, 4], 954)\n",
      "(u'd\\xe9sirais_X', (6, {2008: 5, 2009: 6, 2007: 4}))\n",
      "(u'd\\xe9sirais_X', [4, 5, 6])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "361\n",
      "(u'decapitate_ADJ', [1, 2, 4], 954)\n",
      "(u'eigens_X', (13, {2008: 5, 2009: 3, 2007: 2}))\n",
      "(u'eigens_X', [2, 5, 3])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "316\n",
      "(u'egyenl\\u0151_NOUN', [1, 2, 5], 912)\n",
      "(u'fawn', (12211, {2008: 12211, 2009: 7427, 2007: 7858}))\n",
      "(u'fawn', [7858, 12211, 7427])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "366\n",
      "(u'Frilly_ADJ', [1, 2, 4], 954)\n",
      "(u'Glendhu', (47, {2008: 47, 2009: 2, 2007: 36}))\n",
      "(u'Glendhu', [36, 47, 2])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "245\n",
      "(u'geschildert_X', [1, 2, 5], 912)\n",
      "(u'Handschlag', (9, {2003: 1, 2005: 1, 2006: 7}))\n",
      "(u'Handschlag', [1, 1, 7])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "280\n",
      "(u'HEINDEL_NOUN', [1, 2, 4], 954)\n",
      "(u'iugement', (84, {2008: 5, 2009: 27, 2007: 84}))\n",
      "(u'Inculcation', [5, 3, 2])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "277\n",
      "(u'inaesthetic', [1, 2, 4], 954)\n",
      "(u'Jojo', (7363, {2008: 1727, 2009: 1515, 2007: 3734}))\n",
      "(u'Jojo', [3734, 1727, 1515])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "108\n",
      "(u'JANIFER_NOUN', [1, 2, 5], 912)\n",
      "(u'Kagesue', (391, {2008: 8, 2006: 19, 2007: 3}))\n",
      "(u'Kagesue', [19, 3, 8])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "203\n",
      "(u'kartus', [1, 2, 4], 954)\n",
      "(u'lenitives', (27, {2008: 27, 2009: 25, 2007: 10}))\n",
      "(u'lenitives', [10, 27, 25])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "273\n",
      "(u'Loir_ADJ', [1, 2, 4], 954)\n",
      "(u'Mebby', (1330, {2008: 1330, 2009: 303, 2007: 1024}))\n",
      "(u'Mebby', [1024, 1330, 303])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "430\n",
      "(u'mettrons', [1, 2, 5], 912)\n",
      "(u'nicing', (14, {2008: 7, 2009: 5, 2007: 2}))\n",
      "(u'nicing', [2, 7, 5])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "223\n",
      "(u'Nerwinde', [1, 2, 4], 954)\n",
      "(u'onsite_ADJ', (254, {2008: 213, 2009: 254, 2007: 140}))\n",
      "(u'onsite_ADJ', [140, 213, 254])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "224\n",
      "(u'opthalmic_ADJ', [1, 2, 4], 954)\n",
      "(u'Privet_ADJ', (6, {2008: 5, 2009: 4, 2007: 2}))\n",
      "(u'Pyrasus_NOUN', [14, 9, 6])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "501\n",
      "(u'Presidentl_NOUN', [1, 2, 5], 912)\n",
      "(u\"Quinn'll_NOUN\", (11, {2005: 1, 2006: 2, 2007: 1}))\n",
      "(u\"Quinn'll_NOUN\", [1, 2, 1])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "39\n",
      "(u'quun_X', [1, 2, 4], 954)\n",
      "(u'Refidence', (10, {2009: 10, 1996: 1, 2005: 1}))\n",
      "(u'Refidence', [1, 1, 10])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "306\n",
      "(u'RESTRICTED_X', [1, 2, 5], 912)\n",
      "(u'syringes_NOUN', (1932, {2008: 1932, 2009: 1842, 2007: 1613}))\n",
      "(u'syringes_NOUN', [1613, 1932, 1842])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "774\n",
      "(u'Surround_ADP', [1, 2, 5], 912)\n",
      "(u'Took_.', (7, {2008: 7, 2009: 5, 2007: 3}))\n",
      "(u'Took_.', [3, 7, 5])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "492\n",
      "(u'tanacsado_X', [1, 2, 4], 954)\n",
      "(u'underlyingly', (132, {2008: 132, 2009: 21, 2007: 52}))\n",
      "(u'underlyingly', [52, 132, 21])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "192\n",
      "(u'unlosable', [1, 2, 5], 912)\n",
      "(u'virtuoso_VERB', (21, {2008: 12, 2009: 20, 2007: 15}))\n",
      "(u'virtuoso_VERB', [15, 12, 20])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "204\n",
      "(u'volucer_X', [1, 2, 4], 954)\n",
      "(u'whitetoothed', (36, {2008: 36, 2009: 25, 2007: 30}))\n",
      "(u'wome_DET', [3, 3, 4])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "196\n",
      "(u'woirid', [1, 2, 4], 954)\n",
      "(u'xylem', (122, {2008: 17, 2009: 122, 2007: 26}))\n",
      "(u'xylem', [26, 17, 122])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "14\n",
      "(u'xxvjth', [1, 2, 5], 912)\n",
      "(u'Yol_X', (53, {2008: 11, 2009: 3, 2007: 53}))\n",
      "(u'Yol_X', [53, 11, 3])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "50\n",
      "(u'Yakim_NOUN', [1, 2, 5], 912)\n",
      "(u'Ziklag_NOUN', (196, {2008: 79, 2009: 196, 2007: 85}))\n",
      "(u'Zinik', [1, 4, 7])\n",
      "((1, 2, 4), 954)\n",
      "221\n",
      "2\n",
      "37\n",
      "(u'Znanosti', [1, 2, 4], 954)\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "#step 4\n",
    "#match ar\n",
    "#detail info (word,(max,dic))\n",
    "file_names=[\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\"]\n",
    "for file_name_letter in  file_names:\n",
    "    textFile = sc.textFile(\"hdfs://student13-x1:9000/ngram/complete/googlebooks-eng-fiction-all-1gram-20120701-\"+file_name_letter);\n",
    "    def mapfunction(line):\n",
    "        word=line.split(\"\\t\")[0]\n",
    "        year=int(line.split(\"\\t\")[1])\n",
    "        wcount=int(line.split(\"\\t\")[2])\n",
    "        a=word\n",
    "        b=(wcount,{year:wcount})\n",
    "        return (a,b)\n",
    "    def reducefuntion(a,b):\n",
    "        c={}\n",
    "        a[1].update(b[1])\n",
    "        len_a=len(a[1].keys())\n",
    "        key_list=sorted(a[1].keys())\n",
    "        c[key_list[len_a-1]]=a[1][key_list[len_a-1]]\n",
    "        c[key_list[len_a-2]]=a[1][key_list[len_a-2]]    \n",
    "        c[key_list[len_a-3]]=a[1][key_list[len_a-3]]    \n",
    "        return (max(a[0],b[0]),c)\n",
    "    def mapfunction2(line):\n",
    "        list_y=sorted(line[1][1].keys())\n",
    "        if len(list_y)<=2 or line[1][0]>=89794:\n",
    "            return (line[0],[0,0,0])\n",
    "        listv=[]\n",
    "\n",
    "        listv.append(line[1][1][list_y[0]])\n",
    "        listv.append(line[1][1][list_y[1]])\n",
    "        listv.append(line[1][1][list_y[2]])\n",
    "\n",
    "        return (line[0],listv)\n",
    "    counts = textFile.map(mapfunction);\n",
    "    counts = counts.reduceByKey(reducefuntion);\n",
    "    print counts.collect()[0]\n",
    "\n",
    "    counts =counts.map(mapfunction2)\n",
    "    counts = counts.reduceByKey(lambda a,b:a)\n",
    "    counts=counts.collect()\n",
    "\n",
    "    print counts[0]\n",
    "\n",
    "    limit=700\n",
    "    arFile = sc.textFile(\"hdfs://student13-x1:9000/ngram_results_1/ar_rules_11025_u10000\");\n",
    "    arSet=arFile.collect()\n",
    "    print arSet[0]\n",
    "    print len(arSet)\n",
    "    arSet_limit=[]\n",
    "    for item in arSet:\n",
    "        import ast\n",
    "        ar=ast.literal_eval(item)\n",
    "        if ar[1]>=limit:\n",
    "            arSet_limit.append(ar)\n",
    "    print len(arSet_limit)\n",
    "    wlist=[]\n",
    "    for item in counts:\n",
    "        for ar in arSet_limit:\n",
    "            if ar[0]==tuple(item[1]):\n",
    "                neww=(item[0],item[1],ar[1])\n",
    "                wlist.append(neww)\n",
    "    print len(wlist)\n",
    "    print wlist[0]\n",
    "    sc.parallelize(wlist).saveAsTextFile(\"hdfs://student13-x1:9000/ngram_results_2/final_results/results_limit700_\"+file_name_letter);\n",
    "print \"finished\"\n",
    "#for item in wlist:\n",
    "#    print item\n",
    "\n",
    "#counts = counts.sortBy(lambda a: a[1][0],ascending=True).take(5512730-11025);\n",
    "\n",
    "#counts=sc.parallelize(counts).saveAsTextFile(\"hdfs://student13-x1:9000/ngram_results/notin_11025_detailed\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7549\n",
      "(u'Ambulant_ADJ', [1, 2, 4], 954)\n",
      "(u'ahsorhing', [1, 2, 4], 954)\n",
      "(u'anqther', [1, 2, 4], 954)\n",
      "(u'awys', [1, 2, 4], 954)\n",
      "(u'allit_VERB', [1, 2, 4], 954)\n",
      "(u'anflygning_VERB', [1, 2, 4], 954)\n",
      "(u'A32_NUM', [1, 2, 4], 954)\n",
      "(u'arrebata', [1, 2, 4], 954)\n",
      "(u'allaway', [1, 2, 5], 912)\n",
      "(u'Arraign_VERB', [1, 2, 4], 954)\n"
     ]
    }
   ],
   "source": [
    "textFile = sc.textFile(\"hdfs://student13-x1:9000/ngram_results/final_results/*/*\");\n",
    "counts=textFile.collect();\n",
    "print len(counts)\n",
    "for i in range(0,10):\n",
    "    print counts[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n",
      "('124', 59)\n",
      "('125', 24)\n",
      "('0', 0)\n",
      "p\n",
      "('0', 0)\n",
      "('124', 230)\n",
      "('125', 107)\n",
      "w\n",
      "('0', 0)\n",
      "('124', 74)\n",
      "('125', 51)\n",
      "g\n",
      "('0', 0)\n",
      "('124', 102)\n",
      "('125', 31)\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "count_124=0\n",
    "count_125=0\n",
    "file_names=[\"o\",\"p\",\"w\",\"g\"]\n",
    "for file_name_letter in  file_names:\n",
    "    textFile = sc.textFile(\"hdfs://student13-x1:9000/ngram/complete/googlebooks-eng-all-1gram-20120701-\"+file_name_letter);\n",
    "    def mapfunction(line):\n",
    "        word=line.split(\"\\t\")[0]\n",
    "        year=int(line.split(\"\\t\")[1])\n",
    "        wcount=int(line.split(\"\\t\")[2])\n",
    "        a=word\n",
    "        b=(wcount,{year:wcount})\n",
    "        return (a,b)\n",
    "    def reducefuntion(a,b):\n",
    "        a[1].update(b[1])\n",
    "        return (max(a[0],b[0]),a[1])\n",
    "\n",
    "\n",
    "    counts = textFile.map(mapfunction);\n",
    "    counts = counts.reduceByKey(reducefuntion);\n",
    "\n",
    "    unit=10000\n",
    "    def find_nearest(array,value):\n",
    "        minabs=9999999\n",
    "        result=0\n",
    "        for v in array:\n",
    "            tempabs=abs(v-value)\n",
    "            if tempabs<minabs:\n",
    "                minabs=tempabs\n",
    "                result=v\n",
    "        return result\n",
    "    def get2counts(dic,maxv):\n",
    "        year4=-1\n",
    "        for key in dic.keys():\n",
    "            if maxv<89794:\n",
    "                if dic[key]>=maxv:\n",
    "                    year4=key\n",
    "                    break\n",
    "            elif dic[key]>=89794:\n",
    "                year4=key\n",
    "                break\n",
    "                \n",
    "        year3=year4-5\n",
    "        year2=year3-10\n",
    "        year1=year2-10\n",
    "        year3=find_nearest(dic.keys(),year3)\n",
    "        year2=find_nearest(dic.keys(),year2)\n",
    "        year1=find_nearest(dic.keys(),year1)\n",
    "        value3=dic[year3]/unit\n",
    "        value2=dic[year2]/unit\n",
    "        value1=dic[year1]/unit\n",
    "        return (value1,value2,value3)\n",
    "    def mapfunction2(line):\n",
    "        word=line[0]\n",
    "        maxcount=line[1][0]\n",
    "\n",
    "        dic= line[1][1]\n",
    "        value_2=get2counts(dic,maxcount)\n",
    "        return (line[0],value_2)\n",
    "\n",
    "    counts = counts.map(mapfunction2);\n",
    "    counts=  counts.reduceByKey(lambda a,b:a);\n",
    "    def isequal(a,b):\n",
    "        for i in range(0,3):\n",
    "            if a[i]!=b[i]:\n",
    "                return False\n",
    "        return True\n",
    "    def mapfunction3(line):\n",
    "        if isequal(line[1],(1,2,4)):\n",
    "            return ('124',1)\n",
    "        if isequal(line[1],(1,2,5)):\n",
    "            return ('125',1)\n",
    "        return ('0',0)\n",
    "    def reducefunction3(a,b):\n",
    "        return a+b\n",
    "    counts = counts.map(mapfunction3);\n",
    "    counts=  counts.reduceByKey(reducefunction3);\n",
    "    print file_name_letter\n",
    "    print counts.collect()[0]\n",
    "    if len(counts.collect())>1:\n",
    "        print counts.collect()[1]\n",
    "    if len(counts.collect())>2:\n",
    "        print counts.collect()[2]\n",
    "print \"finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "37\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "#for presetation\n",
    "#z in the step4\n",
    "print \"start\"\n",
    "file_names=[\"z\"]\n",
    "for file_name_letter in  file_names:\n",
    "    textFile = sc.textFile(\"hdfs://student13-x1:9000/ngram/complete/googlebooks-eng-fiction-all-1gram-20120701-\"+file_name_letter);\n",
    "    def mapfunction(line):\n",
    "        word=line.split(\"\\t\")[0]\n",
    "        year=int(line.split(\"\\t\")[1])\n",
    "        wcount=int(line.split(\"\\t\")[2])\n",
    "        a=word\n",
    "        b=(wcount,{year:wcount})\n",
    "        return (a,b)\n",
    "    def reducefuntion(a,b):\n",
    "        c={}\n",
    "        a[1].update(b[1])\n",
    "        len_a=len(a[1].keys())\n",
    "        key_list=sorted(a[1].keys())\n",
    "        c[key_list[len_a-1]]=a[1][key_list[len_a-1]]\n",
    "        c[key_list[len_a-2]]=a[1][key_list[len_a-2]]    \n",
    "        c[key_list[len_a-3]]=a[1][key_list[len_a-3]]    \n",
    "        return (max(a[0],b[0]),c)\n",
    "    def mapfunction2(line):\n",
    "        list_y=sorted(line[1][1].keys())\n",
    "        if len(list_y)<=2 or line[1][0]>=89794:\n",
    "            return (line[0],[0,0,0])\n",
    "        listv=[]\n",
    "\n",
    "        listv.append(line[1][1][list_y[0]])\n",
    "        listv.append(line[1][1][list_y[1]])\n",
    "        listv.append(line[1][1][list_y[2]])\n",
    "\n",
    "        return (line[0],listv)\n",
    "    counts = textFile.map(mapfunction);\n",
    "    counts = counts.reduceByKey(reducefuntion);\n",
    "    #print counts.collect()[0]\n",
    "\n",
    "    counts =counts.map(mapfunction2)\n",
    "    counts = counts.reduceByKey(lambda a,b:a)\n",
    "    counts=counts.collect()\n",
    "\n",
    "    #print counts[0]\n",
    "\n",
    "    limit=700\n",
    "    arFile = sc.textFile(\"hdfs://student13-x1:9000/ngram_results_1/ar_rules_11025_u10000\");\n",
    "    arSet=arFile.collect()\n",
    "    #print arSet[0]\n",
    "    #print len(arSet)\n",
    "    arSet_limit=[]\n",
    "    for item in arSet:\n",
    "        import ast\n",
    "        ar=ast.literal_eval(item)\n",
    "        if ar[1]>=limit:\n",
    "            arSet_limit.append(ar)\n",
    "    #print len(arSet_limit)\n",
    "    wlist=[]\n",
    "    for item in counts:\n",
    "        for ar in arSet_limit:\n",
    "            if ar[0]==tuple(item[1]):\n",
    "                neww=(item[0],item[1],ar[1])\n",
    "                wlist.append(neww)\n",
    "    print len(wlist)\n",
    "    #print wlist[0]\n",
    "    sc.parallelize(wlist).saveAsTextFile(\"hdfs://student13-x1:9000/ngram_results_5/final_results/results_limit700_\"+file_name_letter);\n",
    "print \"finished\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
